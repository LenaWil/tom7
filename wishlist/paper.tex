\documentclass[twocolumn]{article}
\usepackage[top=1.1in, left=0.85in, right=0.85in]{geometry}

% \usepackage{eclbkbox}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{amscd}
% \usepackage{xy}
\usepackage{graphicx}
% \usepackage{fancyhdr}
% \usepackage{color}
% \usepackage[dark,all,bottom,landscape,timestamp]{draftcopy}
% \usepackage{everypage}

% \usepackage{ulem}
% go back to italics for emphasis, though
% \normalem

\begin{document} 

\title{What words ought to exist? \\
       {\normalsize Coining with coinduction}}
\author{Dr.~Tom~Murphy~VII~Ph.D.\thanks{
Copyright \copyright\ 2011 the Regents of the Wikiplia
Foundation. Appears in SIGBOVIK 2011 with the blessing of the
Association for Computational Heresy; {\em IEEEEEE!} press,
Verlag-Verlag volume no.~0x40-2A.
\yen 0.00}
}


\renewcommand\>{$>$}
\newcommand\<{$<$}

\date{3 March 2011}

\maketitle

\begin{abstract}
This paper is an earnest attempt to answer the following question
scientifically: What words ought to exist?
\end{abstract}

\vspace{1em}
{\noindent \small {\bf Keywords}:
 computational cryptolexicography, n-Markov models, coinduction
}

\section*{Introduction}
During a recent high-stakes game of Scrabble-brand Crossword
Puzzle\footnote{Scrabble is a registered trademark of Hasbro
  Inc./Milton Bradley, and Mattel/JW Spear \& Sons plc.} I had what
could only be described as a killer bingo word (all 7 tiles) that,
after careful study, I determined could not be placed anywhere on the
board. Later in that same game, I had another sequence of letters that
just totally seemed like it should be able to make some long-ass
words, like for example ``oilsoap'' which turns out is not a legal
Scrabble word.\!\footnote{There are actually no 7-letter words that
  can be made from these letters. Don't even bother. Even if playing
  off an existing letter on the board, the best we can do are the
  non-bingos ``topsoil,'' ``topsail,'' or ``poloist'' with an
  available {\it t}.} This naturally made me frustrated and I wanted
to do something about it. Why can't ``oilsoap'' be a word? Or
``loopsia''? Words are introduced into the lexicon all the time. My
first reaction of course was to make an online version of Scrabble
where all words are legal. This is called Scrallbe (where they can
{\it all be} words!\footnote{As of 2011, the official Scrabble slogan
  is ``every word's a winner!'' which is clearly false.}) This is
available at {\tt http://snoot.org/toys/scrallbe}, and is pretty
boring, I gotta be honest (Figure~\ref{fig:scrallbe}).

\begin{figure}
\begin{center}
\includegraphics[width=0.75 \linewidth]{scrallbe-screenshot}
\end{center}\vspace{-0.1in}
\caption{In-progress Scrallbe game, 753 points.}
\label{fig:scrallbe}
\end{figure}

The thing is, it's just more fun when some words aren't words. Think
about it: If all words were real, then you could never make a really
devastatingly successful challenge in Scrabble that like, rocked the
whole household and turned a formerly casual family games night into
some kind of crying contest. Spelling bees could still exist, because
while no matter what those kids spelled,\!\footnote{Well, we have to
consider the possibility that the kiddo would use a letter that
doesn't exist. In this particular fantasy, grant me also that every
letter also exists, even $\stackrel{\Diamond}{\smile}$.} it would be
a word, it would not necessarily be the {\it right} word, just like
maybe a homophone. There would be fewer bar fights, but probably not
that many fewer. Moreover, iuhwueg nznie a uaohahweih zmbgba bawuyg!

Clearly we need more words, but not all of them. So this raises the
question: What words {\it ought} to exist? This paper explores several
different approaches for scientifically answering this question,
compares the results, and proposes specific words that should be
added, with their meanings.

Disclaimer possibly indicated for SIGBOVIK: The ``research'' contained
herein is 100\% legitimate.\!\footnote{Source code is available at
{\tt http://tom7misc.svn.
sourceforge.net/viewvc/tom7misc/trunk/wishlist/}} I have attempted to
present it in a tutorial style that assumes little mathematical or
computer science background. I have also left off the last {\it S} for
{\it Savings}.

\section{First idea: Wishlist}

My website ``{\sf snoot.org}'' has a number of games on it, including
a Scrabble clone called Scribble\footnote{{\tt
    http://snoot.org/toys/scribble/}} and Boggle clone called
Muddle.\!\footnote{{\tt http://snoot.org/toys/muddle/}} This website
has been running for almost ten years, comprising over 150,000
Scribble games totaling 3.8 million words placed and 628,000 Muddle
games with over 10 million words found. During each game, players
repeatedly attempt to play words that aren't real. The computer
rebukes them, but hope really springs eternal with these people. It's
like they truly deeply wish to break out of the shackles of the
Official Scrabble Players Dictionary.\!\footnote{For the analyses in
  this section that depend on a list of legal words, I actually use a
  modified version of SOWPODS, which is the tournament list used in
  Australia and the UK, and significantly more permissive than the US
  Tournament Word List. Though the modified version is non-canonical,
  I stuck with it because it's what's been in use on the site for
  ten years.} So the first approach to determining what
words ought to exist is to analyze the words that people tried to
play, in order to try to extract the essence of word-yearning.

This analysis is quite straightforward. I took the ten years of logs
files and extracted each attempt to play a word in Scribble or Muddle.
These log files are quite large, so the first step is just to get a
count, for each alleged word, and store those in a more convenient
format. There were 3,572,226 total words attempted\footnote{Here a
  word attempted is the major word of the play. This does not include
  incidental words (typically two-letter ones) formed in the
  perpendicular direction.} in Scribble and 13,727,511 in Muddle. The
most frequent ones appear in Figure~\ref{fig:mostfrequent}. Aside from
the one-letter ones, the most frequent words are legitimate words,
since players have a bias towards attempting words that will not be
rebuked by the computer.

Seeing the words that people wish existed is a simple matter of
filtering out the words that already exist, using the Scrabble
dictionary. (I also filtered out one-letter ``words''. It is easy to
see that no one-letter words should exist, again because of
ambiguities created in spelling bees. Not only when literally spelling
``bees'', but according to the official Scripps National Spelling Bee
rules, the speller may optionally pronounce the word to be spelled
before and after spelling it. So if ``s'' were a word, then the
following ridiculous exchange obtains: Judge: ``S. The letter {\it s}.
Etruscan origin.'' Speller: ``S. S. S.'' and the judge cannot tell if
the speller meant to state the word before and after, or thinks the
word is spelled ``sss''.) 22.3\% of the words attempted in Scribble
and 36.8\% in Muddle were not real. The most frequent ones appear in
Figure~\ref{fig:mostfrequentfake}.

\begin{figure}
\begin{center}
\begin{tabular}{|rl@{\qquad}rl|}
\multicolumn{2}{l}{{\bf \large Scribble}} &
\multicolumn{2}{l}{{\bf \large Muddle}} \\
\hline
Count & Word & Count & Word \\
\hline
45,605  &  a        &     20,412  &  late  \\
42,315  &  i        &     19,405  &  rate  \\
32,499  &  d$^*$    &     19,276  &  dear  \\
12,981  &  in       &     19,049  &  tear  \\
12,851  &  oe       &     19,019  &  date  \\
12,528  &  s$^*$    &     18,771  &  lear  \\
12,207  &  re       &     18,423  &  deal  \\
11,159  &  tv       &     18,231  &  real  \\
10,720  &  jo       &     18,138  &  lead  \\
10,386  &  it       &     18,076  &  tale  \\
10,369  &  et       &     17,969  &  lane  \\
9,659   &  qua      &     17,956  &  sear  \\
9,218   &  xi       &     17,570  &  read  \\
9,099   &  go       &     17,193  &  teal  \\
9,052   &  ow       &     17,170  &  lean  \\
8,801   &  qat      &     17,071  &  dare  \\
8,602   &  aa       &     16,923  &  dale  \\
8,278   &  un       &     16,892  &  seal  \\
8,142   &  en       &     16,806  &  sale  \\
8,005   &  or       &     16,465  &  seat  \\
\hline
\end{tabular}
\end{center}
\caption{Most frequently attempted words in Scribble and Muddle. Asterisks
indicate non-words.}
\label{fig:mostfrequent}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{|rl@{\qquad}rl|}
\multicolumn{2}{l}{{\bf \large Scribble}} &
\multicolumn{2}{l}{{\bf \large Muddle}} \\
\hline
Count & Word & Count & Word \\
\hline
11,159 &  tv         &     16,251 &  dane    \\
4,003  &  ok         &     6,156  &  rane    \\
2,862  &  iraq       &     5,603  &  sare    \\
2,725  &  zen        &     5,576  &  nate    \\
2,448  &  cho        &     4,863  &  mear    \\
1,538  &  viz        &     4,750  &  cale    \\
1,418  &  sdasda     &     4,616  &  nees    \\
1,396  &  von        &     4,568  &  nale    \\
1,136  &  etc        &     4,507  &  fale    \\
878    &  int        &     4,347  &  deat    \\
829    &  june       &     4,263  &  tean    \\
745    &  lp         &     4,251  &  nile    \\
719    &  zion       &     4,160  &  mens    \\
665    &  cia        &     4,087  &  deel    \\
661    &  jim        &     3,851  &  deam    \\
651    &  iraqi      &     3,828  &  dana    \\
648    &  ques       &     3,781  &  beed    \\
542    &  que        &     3,769  &  lans    \\
502    &  tim        &     3,725  &  tade    \\
\hline
\end{tabular}
\end{center}
\caption{Most frequently attempted non-words in Scrabble and Muddle.}
\label{fig:mostfrequentfake}
\end{figure}

There's a clear difference between these two lists. The Scribble list
is dominated by words involving difficult-to-play letters like {\it v}
(there are no legal two-letter {\it v}-words). Most of the words would
probably be acknowledged as real, just not legal in Scribble. The ones
that don't already have meanings, like ``cho'' and ``int'' and ``que''
seem to be pretty good candidates to exist. The Muddle list is all
four-letter words (the minimum allowed length) using common letters.
Other than the ones that are already words, like ``dane'' and ``nile''
and ``mens'' (as in ``mens section'' or ``the powerfuel weapon kills
hard so many mens''), these are all good candidates for words to
exist. Probably if you were playing someone really intense in
Scrabble, and he or she played one of these, and was super deadpan
about it and maybe had caused some crying contests before, and a known
sesquipedalianist, you would let these fly because they look like real
words to me. A point in their favor is that they would be quite
low-scoring words in Scrabble; not a {\it z} or {\it q} to be found.
Even in the Scribble list there's no ``qzkwv'' junk. The effect is
probably due to a few factors: Players are less likely to attempt
obvious non-words, common letters appear more often on the rack and on
the board and so the opportunity to play words like in
Figure~\ref{fig:mostfrequentfake} presents itself more frequently, and
in Muddle, there is no advantage to using unusual letters, except the
joy of being a weirdo. Nonetheless, these lists are surely biased by
the specifics of Scribble and Muddle, and the question at hand is not
just what words ought to exist for the purpose of internet word games,
but for general purposes.

\begin{figure}
\includegraphics[width=\linewidth]{wishlist-cdf}
\vspace{-0.1in}\caption{Cumulative distribution of word frequency.
  Approximately 25,000 different words (y axis) were issued 55 times
  or fewer (x axis). The ``total'' area does not appear much larger
  than its components because this is a log-log plot.}
\label{fig:gamedistribution}
\end{figure}

Another downside is that this method completely ignores the many words
that are attempted only once or a small number of times. Players are
very creative; of the 564,610 unique words attempted, 501,939 of them
aren't real! The vast majority of words are attempted only a handful
of times (Figure~\ref{fig:gamedistribution}). Though those words
individually are not good candidates to exist, like tiny stars wished
upon in the night sky,\!\footnote{Astronomers now agree that stars do
  exist, by the way.} in aggregate they form a significant planetarium
that may tell us what {\it kind} of words people wish existed. For
example, if we saw that the words ``sweeeeeeet'',
``sweeeeeeeeeeeeet'', ``sweeeet'' and ``sweeeeeeeeeeeeeet'' occurred a
few times each, we could infer that people wished that words like
``sweet'' with strictly more than two {\it e}s were real words. They
might even be indifferent to the absolute number of {\it e}s, as long
as there existed some legal variation with more than two {\it e}s.
(This appears to be borne out by data. According to Google's
estimates, the words ``swe$^n$t'' for various medium-sized $n$
(10--20) appear on the Internet with similar frequency. The only
exception is ``sweeeeeeeeeeeeeeeeeeet'', with 19 {\it e}s, which
unexpectedly appears three times as often as 18 or 20 {\it e}s does;
see Figure~\ref{fig:sweet}.) In order to lance these two boils, in the
next section I explore statistical methods for generalizing from lots
of individual examples.

% XXX should really try to get this in the right column, since it
% overflows the margins
\begin{figure}
\begin{tabular}{r|r|l}
17,900,000 & 0 & swt \\
1,060,000 & 1 & swet \\
580,000,000 & 2 & sweet \\
1,310,000 & 3 & sweeet \\
806,000 & 4 & sweeeet \\
509,000 & 5 & sweeeeet \\
283,000$^1$ & 6 & sweeeeeet \\  % spell correction for five
170,000 & 7 & sweeeeeeet \\
115,000 & 8 & sweeeeeeeet \\
75,200 & 9 &  sweeeeeeeeet \\
94,300$^2$ & 10 & sweeeeeeeeeet \\ % spell correction for nine?
51,700 & 11 & sweeeeeeeeeeet \\
37,900 & 12 & sweeeeeeeeeeeet \\
32,000 & 13 & sweeeeeeeeeeeeet \\
25,300 & 14 & sweeeeeeeeeeeeeet \\
24,300 & 15 & sweeeeeeeeeeeeeeet \\
41,000$^3$ & 16 & sweeeeeeeeeeeeeeeet \\ % spell correction for 14
55,000 & 17 & sweeeeeeeeeeeeeeeeet \\
45,000 & 18 & sweeeeeeeeeeeeeeeeeet \\
133,000$^4$ & 19 & sweeeeeeeeeeeeeeeeeeet \\ % for 15
34,800 & 20 &  sweeeeeeeeeeeeeeeeeeeet \\
%
16,100$^5$ & 25 & sweeeeeeeeeeeeeeeeeeeeeeeeet \\ % for weeeeeeeeeeeeeeeeeeeeeeeee t
10,100 & 30 & sweeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
2,800 & 40 &  sweeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
923 & 50 &    sweeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
118 & 75 &    sweeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
38 & 100 &    sweeeeeeeeeeeeeeeeeeeeeeeeee\ldots t  \\
?$^6$ & 200 & sweeeeeeeeeeeeeeeeeeeeeeeeee\ldots t     \\ % word is too long for Google
\end{tabular}
\caption{Frequency of ``swe$^n$t'' on the internet for various $n$, estimated
by Google. Notes: {\bf (1)} Spell correction offered for ``sweeeeet''. {\bf (2, 3, 4)} Spell corrections
offered to e$^{9}$, e$^{14}$ and e$^{15}$ respectively. {\bf (5)} Spell correction offered for ``weeeeeeeeeeeeeeeeeeeeeeeee t'' (?) {\bf (6)} With two hundred {\it e}s, the word is too long for Google, which asks me to ``try using a shorter word.'' Thanks Google, but I already did try the shorter ones.}
\label{fig:sweet}
\end{figure}

\section{Statistical models}

The reason that people are more likely to play words like ``rane'' is
that the letters are common---they appear more often in words, and
more often in the Scrabble bag. But it's not simply a matter of the
frequency of letters; if it were, we would expect to see words like
``eee'' dominating the list, since {\it e} is the most common letter
in English.\!\footnote{Tied for first place with {\it n}, {\it g},
{\it l}, {\it i}, {\it s}, and {\it h}.} People do not play such words
often because they do not {\it seem} like real words. ``oilsoap''
seems more like a word than ``ioaopsl'' to most non-crazy people,
even though they contain the same letters. This is because we have
expectations on what letters are likely to appear next to one
another in words. This section is about modeling expectations on
what letters appear together, and then using that model to generate
the most likely words that don't yet exist.

{\bf Markov chains.}\,
This guy called Andrei Markov had an idea which is pretty obvious in
retrospect, but he had it like a hundred years ago before any of us
were born (probably; if not: you are old), which he didn't call Markov
chains but now they're called Markov chains because I guess in the
hopes that contemporary mathematicians will get stuff named after
their dead selves if they keep the tradition of naming stuff after
dead people alive. The idea is easiest to understand in the context
of the current problem. Suppose we know that the words ``hello'',
``helpful'' and ``felafel'' are the only real words. The following
is a frequency table of how often each letter occurs.

\begin{center}
\begin{tabular}{|r|r|r|r|r|r|r|r|} % {helopfua}
\hline
h   & e   & l   & o   & p   & f   & u   & a    \\
\hline
2   & 4   & 6   & 1   & 1   & 3   & 1   & 1    \\
\hline
\end{tabular}
\end{center}

This tells us that {\it l} is by far the most common letter, so the
most likely word is probably ``l'' or ``llllllll'' or something. A
Markov chain is like a frequency table, but instead of counting
individual letters, we count how often one letter comes {\it after}
another. Here is the Markov chain for those words.

% of the current problem. Suppose we know that the words ``hello'',
% ``helpful'' and ``felafel'' are the only real words. The following

\begin{center}
\begin{tabular}{|c|r|r|r|r|r|r|r|r|} % {helopfua}
\hline
\,  &  {\bf h}   & {\bf e}   & {\bf l}   & {\bf o}   & {\bf p}   & {\bf f}   & {\bf u}   & {\bf a} \\
\hline  %    h     e     l     o     p     f     u     a
{\bf h}   &  0   & 0   & 0   & 0   & 0   & 0   & 0   & 0    \\
\hline
{\bf e}   &  2   & 0   & 0   & 0   & 0   & 2   & 0   & 0    \\
\hline
{\bf l}   &  0   & 4   & 1   & 0   & 0   & 0   & 1   & 0    \\
\hline
{\bf o}   &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline
{\bf p}   &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline
{\bf f}   &  0   & 0   & 0   & 0   & 1   & 0   & 0   & 1    \\
\hline
{\bf u}   &  0   & 0   & 0   & 0   & 0   & 1   & 0   & 0    \\
\hline
{\bf a}   &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline
\end{tabular}
\end{center}

The letters across the top are the ``previous letter'' and the ones
across the left are the ``next letter'' and the box contains the
corresponding count. For example, the pair ``el'' appears four times.
(Pairs of letters are called ``bigrams'' by nerds, some nerd-poseurs,
and Markov who I can't tell if he was a nerd by his picture, because
he does have a pretty austere beard, but also did a lot of math.) One
of the useful things about a Markov chain is that it lets us predict
the next letter that we might see. For example, if we see ``half'',
then the column labeled {\bf f} above tells us that the next letter is
twice as often an {\it e} than a {\it u}, and that no other letters
ever occurred. Typically we think of these as being probabilities
inferred from our observations, so we say there's a 2/3 chance of {\it
e} following {\it f} and a 1/3 chance of {\it u}. Now the word ``llllll''
isn't so likely any more, because there's only a 1/4 chance of the
next letter being {\it l} once we see {\it l}.

Words are not just their interiors; it's also important what letters
tend to start and end words. We can do this by imagining that each
word starts and ends with some fake letters, and include those in the
Markov chain. Let's use {\bf \<} for the start symbol and {\bf \>} for
the end. So we pretend we observed ``\<hello\>'', ``\<helpful\>'', and
``\<felafel\>''. Speaking of which, could you imagine if there were such
a thing as a helpful felafel? Would you eat it? Because then it
probably can't help you any more, except to get fat.

\begin{center}
\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|} % {helopfua}
\hline
\,  &  {\bf \<}    & {\bf h}   & {\bf e}   & {\bf l}   & {\bf o}   & {\bf p}   & {\bf f}   & {\bf u}   & {\bf a}    \\
\hline  %  \<     h     e     l     o     p     f     u     a
{\bf h} &  2  &  0   & 0   & 0   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf e} &  0  &  2   & 0   & 0   & 0   & 0   & 2   & 0   & 0    \\
\hline    
{\bf l} &  0  &  0   & 4   & 1   & 0   & 0   & 0   & 1   & 0    \\
\hline    
{\bf o} &  0  &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf p} &  0  &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf f} &  1  &  0   & 0   & 0   & 0   & 1   & 0   & 0   & 1    \\
\hline    
{\bf u} &  0  &  0   & 0   & 0   & 0   & 0   & 1   & 0   & 0    \\
\hline    
{\bf a} &  0  &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf \>} &  0  &  0   & 0   & 2   & 1   & 0   & 0   & 0   & 0    \\
\hline
\end{tabular}
\end{center}

We just added these like other letters, but since the beginning symbol
{\bf \<} never occurs after other letters, we don't need a row for it
(it would be all zeroes), and similarly since no letters ever follow
{\bf \>} we don't need a column for it. Now the word ``lllll'' is
impossible because no words start with {\it l}.

It basically makes sense to consider the probability of a whole word
to be the chance of simultaneously seeing each pair of letters in it,
which is just the product of all the probabilities. So the word
``hel'' is $2/3$ (for {\bf \<h}) $\times$ 2/2 (for {\bf he}) $\times$
4/4 (for {\bf el}) $\times$ 2/6 (for {\bf l\>}), which is $0.222$.
These are the most likely words overall (I discuss how to generate
such lists in Section~\ref{sec:coin}):

\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
22.2\%  &  hel      &      2.5\%  &  helpfel  \\
11.1\%  &  helo     &      2.5\%  &  helafel  \\
 7.4\%  &  fel      &      1.9\%  &  fulo     \\
 3.7\%  &  hell     &      1.9\%  &  hello    \\
 3.7\%  &  felo     &      1.2\%  &  fell     \\
 3.7\%  &  ful      &      1.2\%  &  helafelo \\
\end{tabular}
\end{center}

This is pretty good. These words resemble the ones we observed to build
the Markov chain, but are novel. I think helafelo is a pretty rad word,
right?

\begin{figure}
\includegraphics[width=\linewidth]{coin1}
\caption{Markov chain for the SOWPODS word list, where darker squares
indicate higher probability. The darkest is the transition from {\it q}
to {\it u} ($98\%$), which is not surprising.}
\label{fig:sowpodsbigrams}
\end{figure}

The next step is to build a Markov chain for a list of real words and
see what results. I built one for the SOWPODS word list, which results
in the table in Figure~\ref{fig:sowpodsbigrams}. These
are the most likely words, with real words filtered out:

% XXX improve layout
\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
4.99\%  & s     &   0.17\%  & y   \\
1.75\%  & d     &   0.17\%  & p   \\
0.95\%  & g     &   0.16\%  & a   \\
0.55\%  & c     &   0.16\%  & n   \\
0.43\%  & r     &   0.15\%  & ps  \\
0.42\%  & t     &   0.13\%  & ms  \\
0.40\%  & e     &   0.13\%  & ts  \\
0.35\%  & m     &   0.13\%  & ds  \\
0.32\%  & ss    &   0.11\%  & hy  \\
0.20\%  & rs    &   0.11\%  & k   \\
0.19\%  & h     &   0.11\%  & ng  \\
0.18\%  & l     &   0.11\%  & ly  \\
\end{tabular}
\end{center}


Ugh, poop city! Actually, it turns out that when you see enough words,
you see enough pairs that all sorts of junk looks likely. For example,
``ng'' is easily explained by many words starting with {\it n}, {\it
g} often following {\it n}, and many words ending with {\it g}. Even
though each pair makes sense, the whole thing doesn't look like a word,
because we expect to at least see a vowel at some point, for one thing.

There is a standard solution to this problem, which is to generalize
the Markov chain to keep more than one letter of history. So instead
of just tallying how often {\it g} follows {\it n}, we count how often
{\it g} follows {\it in} (and any other pair of letters).\!\footnote{
  The details are straightforward, except possibly that we now imagine
  each word to start with two (or in general, $n$) copies of the start
  symbol, so that we see ``\<\<helpful\>''. The column corresponding
  to the history {\bf \<\<} tells us the frequency of letters that
  start words, and for example the column {\bf \<h} tells us the
  frequency of letters that follow {\it h} when it appears at the
  start of a word. We do not need to repeat the ending character \>
  because once we see it, we never do anything but end the word.} This
makes the table pretty large, so you'll just have to look at
Figure~\ref{fig:sowpodsbigrams} again and imagine it being 28 times
wider. But the good news is that it invents much better words:

\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
\multicolumn{4}{c}{Markov chain with $n=2$.} \\
\hline
.709\% & ing     &   .110\% & le    \\
.248\% & ses     &   .107\% & der   \\
.169\% & des     &   .107\% & ove   \\
.154\% & nes     &   .101\% & gly   \\
.140\% & sts     &   .088\% & hy    \\
.131\% & se      &   .085\% & ung   \\
.128\% & ings    &   .083\% & cy    \\
.126\% & ded     &   .081\% & pres  \\
.117\% & cal     &   .080\% & pers  \\
\end{tabular}
\end{center}

These are even, like, pronounceable. The best news is that they keep
getting better the more history we keep:

\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
\multicolumn{4}{c}{Markov chain with $n=3$.} \\
\hline
.109\% & des     &   .038\%  & ent     \\
.078\% & pers    &   .036\%  & dist    \\
.076\% & cal     &   .035\%  & ble     \\
.062\% & pres    &   .035\%  & ches    \\
.045\% & nons    &   .034\%  & gly     \\
.044\% & ress    &   .034\%  & inted   \\
.042\% & ing     &   .034\%  & dists   \\
.040\% & pred    &   .033\%  & lity    \\
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
\multicolumn{4}{c}{Markov chain with $n=4$.} \\
\hline
.045\% & unders  &   .017\% & heters  \\
.034\% & dising  &   .016\% & sters   \\
.029\% & pers    &   .015\% & stic    \\
.028\% & cally   &   .014\% & pering  \\
.023\% & inted   &   .013\% & dises   \\
.020\% & heter   &   .013\% & ching   \\
.019\% & tric    &   .012\% & shing   \\
.018\% & ster    &   .012\% & dest    \\
.018\% & hier    &   .011\% & teless  \\
.018\% & unded   &   .011\% & resis   \\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
\multicolumn{4}{c}{Markov chain with $n=5$.} \\
\hline
\multicolumn{4}{l}{{\tt GetTempFileName failed with error 5}} \\
\end{tabular}
\end{center}

With four letters of history, the words produced are quite good! (The
results at $n=5$ are somewhat disappointing since the program crashes
from running out of memory. The table at $n=5$ would have over 481
million entries.) Many of these seem like real words. Some even
suggest meaning because they contain common morphemes. To make the
case that these are not just real-looking words but characteristic of
the English language, compare the results of the same algorithm on the
dictionary from the Italian language edition of Scrabble, which is
probably called {\it Scrabblizzimo!} (Figure~\ref{fig:italian}).
Italian is lexicographically a more compact language than English
(Figure~\ref{fig:italianbigrams}); there are only 21 letters (outside
of occasional interlopers in loan words like {\it jeans} and {\it
  taxi}). Moreover, even though the dictionary contains 585,000 words
(twice as many as English), the probabilities of observing these
non-words are much higher than the most likely English ones.

\begin{figure}
\includegraphics[width=\linewidth]{italiancoin1}
\caption{Markov chain for the Italian language. Again darker cells
  indicate higher probability. Italian has more lexicographic
  structure recognizable from bigraphs than English does: Note that
  the extremely rare letters ``j'', ``k'', ``q'', ``w'', ``x'', and
  ``y'' have almost empty rows. ``z'' very frequently follows ``z'',
  as in {\it pizza}. Words almost always end in a vowel.}
\label{fig:italianbigrams}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
.137\%  &  ammo       &   .026\%  &  rino      \\
.071\%  &  rice       &   .025\%  &  diste     \\
.061\%  &  rico       &   .024\%  &  risti     \\
.055\%  &  este       &   .023\%  &  disci     \\
.053\%  &  scono      &   .022\%  &  riasse    \\
.049\%  &  immo       &   .022\%  &  riassi    \\
.047\%  &  assero     &   .021\%  &  cate      \\
.047\%  &  scano      &   .019\%  &  rite      \\
.038\%  &  rammo      &   .019\%  &  cando     \\
.034\%  &  cata       &   .018\%  &  riassero  \\
.034\%  &  assimo     &   .018\%  &  riassimo  \\
.032\%  &  riate      &   .018\%  &  dete      \\
.032\%  &  disce      &   .018\%  &  disca     \\
.030\%  &  esti       &   .017\%  &  risca     \\
.029\%  &  rica       &   .017\%  &  cente     \\
.028\%  &  endo       &   .016\%  &  acci      \\
.027\%  &  dissimo    &   .015\%  &  centi     \\
.026\%  &  rici       &   .015\%  &  girono    \\
\end{tabular}
\end{center}
\caption{Most probable words induced by the Markov Markov chain for
  the Italian language ($n=4$).}
\label{fig:italian}
\end{figure}


\subsection{Usage-weighted methods}

One criticism of this approach is that it considers every word in the
word list to be equally important.\!\footnote{In fact, the common part
  of words with many different conjugations is in essence counted many
  times. This means {\it ornithology} in its six different forms
  contributes six times as much to our model as the word {\it the}!} I
object on the philosophical grounds that some words that already exist
{\em ought to exist more} than other words that already exist. For
example, {\it congenital} is a much nicer word than the plain ugly
{\it congenial}, and is reflected by the fact that {\it congenial} is
used five times more frequently than {\it
  congenial}.\!\footnote{14,200,000 times to 2,820,000, on the
  Internet, according to Google.} In this section, we produce Markov
models of words weighted by the frequency with which people tend to
use them. This is just a simple matter of training the model on some
natural language corpus (with many occurrences of each word, or no
occurrences of unpopular words) rather than a flat list of all alleged
words.

{\bf Facebook.}\ Since the best research is intensely navel-gazing, I started by 
analyzing a corpus of my own writing, specifically my Facebook status
updates since March~2006. There were 1,386 status updates containing
such jems as ``Tom Murphy VII thinks mathfrak is straight ballin''
and ``Tom Murphy VII global L\_50 reused for unused\_36166!!''. The
most likely words with $n=4$:

\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
\multicolumn{4}{c}{My Facebook status updates, $n=4$.} \\
\hline
.252\% &  pittsburgh   & .097\% &  can't    \\   
.209\% &  steelers     & .083\% &  i'm      \\   
.209\% &  sfo          & .083\% &  icfp     \\   
.195\% &  it's         & .083\% &  app      \\   
.125\% &  bdl          & .069\% &  x        \\   
.111\% &  sigbovik     & .069\% &  drunj    \\   
.109\% &  facebook     & .069\% &  g        \\   
.097\% &  mic          & .061\% &  ther     \\   
.097\% &  s            & .055\% &  doesn't  \\
\end{tabular}
\end{center}

This is the worst. Not only does it contain loads of one-letter words that
we have already determined are verboten,\!\footnote{Note that since $n=4$ these
words have to actually appear in status updates to have nonzero probability for
this list. ``g'' is explained by frequent occurrences of ``e.g.'', for example.}
but the rest are just non-words that I tend to use like the names of cities,
prestigious conferences, or IATA airport codes. The main problem is that there
is simply not enough data from which to generalize.

{\bf Wikipedia.}\ I tried again, but with Wikipedia, using a snapshot
of the English site from June~2009. This is 23 gigabytes of data, most
of it expository text composed by native speakers, plus bathroom humor
vandalism. The list produced by this analysis is much better, though it
contains artifacts from non-English Wiki language used in articles. The
unabridged list appears in the appendix; my hand-selected favorites:

\begin{center}
\begin{tabular}{rl@{\quad\quad}rl}
\multicolumn{4}{c}{English Wikipedia, $n=3$.}     \\
\hline
.0287\% & smally      &     .00518\% & reporth    \\
.0156\% & websity     &     .00484\% & delection  \\
.0156\% & stude       &     .00459\% & grounty    \\
.0124\% & chool       &     .00437\% & betweek    \\
.0120\% & fontry      &     .00431\% & fination   \\
.0102\% & undex       &     .00388\% & manuary    \\
.0099\% & octory      &     .00360\% & whicle     \\
.0096\% & coibot      &     .00262\% & stategory  \\
.0084\% & footnot     &                           \\
\end{tabular}
\end{center}







Lots of these could be the names of tech startups or Pok\'emon.

% laplace smoothing?

\subsection{Coining words with coinduction} \label{sec:coin}

In the earlier sections I blithely produced tables of the most
probable words according to an $n$-Markov chain. It is not obvious how
to do this (or that it is even possible), so I explain the algorithm
in this section. It's safely skippable, I mean if you don't want to
know about a pretty cool algorithm that's not that complicated and
might even be new, plus {\em dual math}.

Computing the probability of an individual word is easy. We prefix it
with $n$ copies of the start symbol \<, suffix it with a single \>,
and then look up the probability of each symbol given its $n$
preceding symbols in the table, and multiply those all together. We
can compute the probability of any word this way. The problem with
sorting all of the possible words by their probabilities is that there
are an infinite number of them. We can't just look at short words
first, either, because for example the word ``thethethe'' is many
times more likely ($p = 6.08\times 10^{-11}$) than the shorter
``qatzs'' ($9.07\times 10^{-12}$).

The solution is to use coinduction. Most people remember induction
from school, maybe, which is the one where you have some base case
like ``0 is even'', and then you prove that all numbers are either
even or odd by assuming ``$n$ is even or odd'' and proving ``$n + 1$
is even or odd''. 

most people remember induction, perhaps 
coinduction: never gonna give you up. once you pop you can't stop.

\section{Special cases}

The empty string?? Is that a word? Could it be? Dude that is blowing my mind.

\section{Backformation} \label{sec:backformation}

The lexicon is generative, in the sense that it's possible to make new
words that are generally acceptable, by following rules. Most people
recognize pluralization of nouns by adding {\it --s} (even for novel
words), or adding prefixes like {\it anti--}. We could investigate
words that ought to exist by the application of rules, such as {\it
  examplelikelikelikelikelikelike}, but I see no straightforward way
to justify the relative strength of such words.

A related way for words to enter the lexicon is by backformation. This
is the reverse of the above process: A word like {\it laser}
(initially an initialism) is legal, and then by running the rules of
English backwards, we start to use {\it lase} as a word (the verb
that a laser most frequently applies). In this section, I attempt
to determine formation rules in English (by simple lexical analysis
of the set of legal words) and then run these rules backwards to find
words that seemingly should already exist.

{\bf Prefixes and suffixes.}\ The first order of business is to find
prefixes and suffixes that are usually modular. The kind of thing
we're tring to find are ``anti--'' and ``--ing''; stuff you can often
add to a word to make a related word. The approach is straightforward.
For each word, consider splitting it at each position. For {\it dealing},
we have {\it d/ealing}, {\it de/aling}, etc. For every such split,
take the prefix (e.g. ``de'') and remainder (``aling''); if the remainder
is still a legal word, then the prefix gets one point. {\it aling} is not
a word so no points here for ``de''. We also do the same thing for suffixes
(using the exact same splits, symmetrically). In this case we'll only get
points for ``--ing'' since {\it deal} is a word. Every time a prefix or
suffix appears we test to see if it is being applied modularly, and
the final score is just the fraction of such times. Here are the ones with
the highest scores:

\begin{verbatim}
1.000000000 -zzyingly    1/1
1.000000000 -zzying      1/1
1.000000000 -zzuolanas   1/1
1.000000000 -zzuolana    1/1
1.000000000 -zzotints    1/1
1.000000000 -zzotintos   1/1
1.000000000 -zzotinto    1/1
1.000000000 -zzotinting  1/1
...
\end{verbatim}

Well, it's good to know that 100\% of the time, you can remove
``--zzotinting'' from a word and it will still be a word. But this
inference is supported by just one observation (the word {\it
  mezzotinting}); there are actually hundreds of such unique prefixes
and suffixes. We need a better list.\!\footnote{The right thing to do
  here is probably to use binomial likelihood rather than the
  scale-independent fraction. But simpler approaches produce pretty
  good lists.} Removing the ones that appear just a single time
doesn't really help that much:

\begin{verbatim}
1.000000000 -zzazzes     3/3
1.000000000 -zzazz       3/3
1.000000000 -zzans       3/3
1.000000000 -zzanim      2/2
1.000000000 -zzan        3/3
1.000000000 -zygotic     3/3
\end{verbatim}

Still bad. Let's turn up the juice to prefixes and suffixes that
appear at least 10 times.

\begin{verbatim}
1.000000000 -wrought    10/10
1.000000000 -writings   12/12
1.000000000 -wraps      10/10
1.000000000 -wrap       11/11
1.000000000 -worms      69/69
1.000000000 -worm       69/69
1.000000000 -working    21/21
\end{verbatim}

Much better! But the next step is going to be to try removing these
prefixes and suffixes from words that have them, to find new words.
Since these have modularity of 100\%, we already know that every
time we apply them, the result will already be a word. So they are
useless for our analysis. Here are the most modular prefixes
and suffixes with modularity {\it strictly less than} 1.

\begin{verbatim}
0.985714286 -makers     69/70
0.985714286 -maker      69/70
0.983606557 -wood       120/122
0.983471074 -woods      119/121
0.982758621 -down       57/58
0.982658960 -works      170/173
0.981818182 -houses     108/110
0.981818182 -house      108/110
0.981132075 kilo-       52/53
0.980752406 -less       1121/1143
0.980743395 over-       2190/2233
0.980000000 -books      49/50
0.980000000 -book       49/50
0.979591837 -proof      48/49
0.979310345 -lessnesses 142/145
0.979069767 -ships      421/430
0.978723404 -lessness   184/188
0.978723404 -board      138/141
0.978494624 -woman      91/93
0.978021978 -women      89/91
0.977528090 -ship       435/445
0.977272727 -manship    43/44
0.976744186 -weeds      84/86
0.976470588 after-      83/85
0.976190476 -manships   41/42
0.976190476 -making     41/42
0.976190476 -craft      41/42
0.976190476 -boats      41/42
0.976190476 -boat       41/42
\end{verbatim}

Wow, now we're talking! The single word that cannot have ``--maker''
removed is {\it comaker}, suggesting that {\it co} should be word
(noun: ``What a comaker makes.'').

Given this list, the next step is to identify potential words that
can be backformed by removing prefixes or adding suffixes from existing
words. Such a string can often be found via multiple prefixes and
suffixes. For example, {\it twing} can be formed by removing ``--ing''
from {\it twinging} (a false positive, since the root word is actually
{\it twinge} in this case) as well as by removing the prefix ``lef--'',
which has modularity of 20\% (including splits such as ``lef/tie'').
Maybe not good justification, but {\it twing} is a pretty good word
anyway.

We define the probability of a word as its Markov probability (with
$n=4$, as this seems to produce the best results), times the
probability that at least one of the potential backformation rules
applies.\!\footnote{As above we only allow backformation rules that
  have at least 10 occurrences, to prevent degeneracy.} Here are the
most likely words by backformation:

\begin{verbatim}
word    prob   most likely backformation rules
==============================================
dises   .023%  para- (0.42) fluori- (0.39) 
               melo- (0.35) bran- (0.31)  
tring   .020%  hams- (0.36) scep- (0.35) 
               bows- (0.33) hearts- (0.29)  
disms   .017%  triba- (0.31) drui- (0.30) 
               bar- (0.27) invali- (0.27)  
ching   .017%  day- (0.86) hot- (0.69) 
               star- (0.51) guillo- (0.50)    
sking   .017%  dama- (0.24) imbo- (0.18) 
               fri- (0.18) atta- (0.17)     
cally   .015%  anti- (0.78) specifi- (0.61)
               magnifi- (0.55) phoni-    
pring   .015%  days- (0.67) heads- (0.62)
               outs- (0.54) ups- (0.51)    
\end{verbatim}

I think that this approach shows promise, but there appear to be a few
problems: Many of these ``rules'' can be explained by bad segmentation
(``heads--'' appearing to be modular, for example, is really just
``head--'' plus ``s'' being a common letter.) Second, I believe the
disjunctive probability of any rule applying is too naive for
determining the score. For example, {\it tions} has almost a thousand
different prefixes that could apply to it; the chance of {\it any one}
of them applying is very nearly 1. But this is actually because ``tions''
is just a common way for a word to end. Legitimate root words to which
many good prefixes are applied cannot be easily distinguished from
common suffixes by this symmetric algorithm. More work is called for
here.

\section{Survey}

On occasion I have been accused of ``overthinking'' problems, whatever
that means. So to compare, I next hazarded a tried and true technique
from grade school, the survey.

I asked a few people who happened to be around, ``What word ought to
exist?'' Most people did not know what to make of this question, and
also, because people seem to revel in the opportunity to get (well
deserved) revenge on me by being disruptive trolls, many of the
answers were designed to be unusable. In order to not reprint
everyone's bullshit---but not introduce bias by selectively removing
data---I discarded random subsets of the data until it did not contain
bullshit any more.

\begin{center}
\begin{tabular}{rl}
Rob: &  etsy, nuuog \\
Chris: &  nurm \\
David: &  wafflucinations \\
Lea: &  hnfff \\
Reed: &  pansepticon \\
Jessica: &  gruntle \\
\end{tabular}
\end{center}

From this we can conclude that 16\% of people wish {\it nurm} were a word, and so on.
These words did not come with definitions, except for {\it gruntle}, which Jessica
gives as ``the opposite of distgruntle''. This is actually already a word, but
it was the inspiration for Section~\ref{sec:backformation}. {\it etsy} is the name
of a popular on-line crafts community so I don't know why Rob would suggest that.
The meaning of {\it wafflucinations} is clear from morphological analysis.



% TODO: length normalization. just take the average (or max?)
% probability for a word of length n, and normalize by that (which means what?)?

% TODO: backformation, stripping dis- and anti- and un- from words
% then checking to see if they're still words?

% TODO: portmanteautally

% ``Scrallbe'' is kind of like god mode for scrabble.

\section{Conclusion}

\subsection{Recommendations}

sweeeeeeeeeeeeeeeeeeet with 19 {\it e}s, which means ``Really sweet.''

``rane'' sounds too much like ``rain'', but ``sare'' has a unique pronunciation and many people seem to think it's already a word. ``cho'' is similarly easy to pronounce and spell. I propose that it be defined as ``A kind of cheese,'' so that we can really nail the new triple entendre on the classic joke.

In this paper I investigated several different ways of answering the
question: What words ought to exist? Each method produces different
words, and some don't work that well, but nonetheless we have several
rich sources of words, each time scientifically justified.

Nobody really likes writing conclusions, so I will just now summarize
the paper by the giving the most likely sentences that I have not
already written.

% XXXX

% \begin{Verbatim}
facebook wall posts, 4-grams
100 top paths:
0.002525504 pittsburgh
0.002093510 steelers
0.002093510 sfo
0.001953943 it's
0.001256106 bdl
0.001116539 sigbovik
0.001099196 facebook
0.000976971 mic
0.000976971 s
0.000976971 can't
0.000837404 i'm
0.000837404 icfp
0.000837404 app
0.000697837 x
0.000697837 drunj
0.000697837 g
0.000616021 ther
0.000558269 doesn't
0.000558269 's
0.000558269 sr
0.000558269 pm
0.000558269 fyi
0.000558269 wtf
0.000558269 k
0.000558269 don't
0.000558269 today's
0.000558269 m
0.000558269 what's
0.000456766 iphone
0.000429438 inted
0.000418702 oh
0.000418702 phl
0.000418702 gonna
0.000418702 pre
0.000418702 zrh
0.000418702 th
0.000418702 h
0.000418702 b
0.000418702 n
0.000418702 ok
0.000418702 d
0.000418702 that's
0.000418702 rex
0.000418702 fanzibar
0.000418702 st
0.000418702 kinda
0.000418702 didn't
0.000355262 cally
0.000334962 brillobox
0.000314027 superburrito
0.000305304 weath
0.000279135 techno
0.000279135 vii


trigrams, including comments:
100 top paths:
0.003013897 it's
0.001856422 ther
0.001704432 i'm
0.001503910 sfo
0.001102867 -
0.001102867 oh
0.001050350 don't
0.001002607 haha
0.001002607 s
0.000902346 bdl
0.000808769 mic
0.000808769 that's
0.000776981 pittsburgh
0.000705812 thes
0.000701825 ok
0.000645063 can't
0.000601564 icfp
0.000601564 k
0.000583335 didn't
0.000507201 runj
0.000501303 g
0.000501303 omg
0.000501303 x
0.000485262 ins
0.000453126 sout
0.000450580 ands
0.000449950 coff
0.000425932 reat
0.000425133 ning
0.000416189 pittle
0.000412468 getty
0.000407151 ster
0.000401043 sr
0.000401043 pm
0.000401043 m
0.000401043 wtf
0.000401043 aw
0.000401043 d
0.000401043 fyi
0.000401043 ple
0.000401043 's
0.000400791 phon
0.000386315 weat
0.000384116 befor
0.000375978 els
0.000374307 als
0.000357782 nothe
0.000354769 app
0.000354769 appy
0.000354422 they're
0.000353750 wate
0.000345727 there's
0.000340508 res
0.000339344 mes
0.000334202 ove
0.000326473 wher
0.000324373 lity
0.000319011 hotos
0.000316613 dea
0.000315777 sigbovik
0.000315105 ver
0.000314965 thand
0.000310607 outh
0.000300782 n


facebook, n = 2
100 top paths:
0.010628671 ing
0.007144197 th
0.005025442 st
0.004291375 re
0.002932937 ne
0.002662094 se
0.002546194 whe
0.002108786 ther
0.001923496 al
0.001669458 con
0.001652820 wor
0.001498222 ch
0.001495485 wis
0.001379778 i'm
0.001336166 jus
0.001302469 sh
0.001280888 yout
0.001244352 fo
0.001225250 le
0.001224422 pre
0.001221357 ar
0.001201881 com
0.001200574 ge
0.001165417 thes
0.001116295 som
0.001108305 fing
0.001104851 ang
0.001102867 -
0.001102706 it's
0.001039704 ist
0.001036886 hing
0.001007930 ou
0.001002607 s
0.000967282 de
0.000932965 en
0.000926568 tor
0.000827260 wat
0.000827151 oh
0.000814235 cor
0.000812961 ned
0.000783696 lis
0.000762020 ple
0.000760162 whis
0.000759898 sed
0.000755396 res
0.000728412 mor
0.000728049 ong
0.000727818 don
0.000720098 cand
0.000710277 ning
0.000708380 ithe
0.000693547 ame
0.000659248 ty
0.000656211 dow
0.000635771 ha
0.000633225 ext
0.000601564 k
% \end{Verbatim}

% Paper ends with a bigram summary of itself.

% Paper ends with a word that is obviously missing a final 's'.


\end{document}
