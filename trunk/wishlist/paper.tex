\documentclass[twocolumn]{article}
\usepackage[top=1.1in, left=0.85in, right=0.85in]{geometry}

% \usepackage{eclbkbox}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{amscd}
% \usepackage{xy}
\usepackage{graphicx}
% \usepackage{fancyhdr}
% \usepackage{color}
% \usepackage[dark,all,bottom,landscape,timestamp]{draftcopy}
% \usepackage{everypage}

% \usepackage{ulem}
% go back to italics for emphasis, though
% \normalem

\begin{document} 

\title{What words ought to exist? \\
       {\normalsize Coining with coinduction}}
\author{Dr.~Tom~Murphy~VII~Ph.D.\thanks{
Copyright \copyright\ 2011 the Regents of the Wikiplia
Foundation. Appears in SIGBOVIK 2011 with the blessing of the
Association for Computational Heresy; {\em IEEEEEE!} press,
Verlag-Verlag volume no.~0x40-2A.
\yen 0.00}
}


\renewcommand\>{$>$}
\newcommand\<{$<$}

\date{3 March 2011}

\maketitle

\begin{abstract}
what do I put here
\end{abstract}

\vspace{1em}
{\noindent \small {\bf Keywords}:
 cryptolexicography, n-Markov models, coinduction
}

\section*{Introduction}
During a recent high-stakes game of Scrabble-brand Crossword
Puzzle\footnote{Scrabble is a registered trademark of Hasbro
Inc./Milton Bradley, and Mattel/JW Spear \& Sons plc.} I had what
could only be described as a killer bingo word (all 7 tiles) that,
after careful study, I determined could not be placed anywhere on the
board. Later in that same game, I had another sequence of letters that
just totally seemed like it should be able to make some long-ass
words, like for example ``oilsoap'' which turns out is not a legal
Scrabble word.\!\footnote{There are actually no 7-letter words that
can be made from these letters. Don't even bother. Even if playing off
an existing letter on the board, the best we can do are the non-bingos
``topsoil,'' ``topsail,'' or ``poloist'' with an available {\it t}.}
This naturally made me frustrated and I wanted to do something about
it. Why can't ``oilsoap'' be a word? Or ``loopsia''? Words are
introduced into the lexicon all the time. My first reaction of course
was to make an online version of Scrabble where all words are legal.
This is available at {\tt http://snoot.org/toys/scrallbe}, and is
pretty boring, I gotta be honest.
% TODO: Screenshot of Scrallbe

The thing is, it's just more fun when some words aren't words. Think
about it: If all words were real, then you could never make a really
devastatingly successful challenge in Scrabble that like, rocked the
whole household and turned a formerly casual family games night into
some kind of crying contest. Spelling bees could still exist, because
while no matter what those kids spelled,\!\footnote{Well, we have to
consider the possibility that the kiddo would use a letter that
doesn't exist. In this particular fantasy, grant me also that every
letter also exists, even $\stackrel{\Diamond}{\smile}$.} it would be
a word, it would not necessarily be the {\it right} word, just like
maybe a homophone. There would be fewer bar fights, but probably not
that many fewer. Moreover, iuhwueg nznie a uaohahweih zmbgba bawuyg!

Clearly we need more words, but not all of them. So this raises the
question: What words {\it ought} to exist? This paper explores several
different approaches for scientifically answering this question,
compares the results, and proposes specific words that should be
added, with their meanings.

Disclaimer possibly indicated for SIGBOVIK: The ``research'' contained
herein is 100\% legitimate.\!\footnote{Source code is available at
{\tt http://tom7misc.svn.
sourceforge.net/viewvc/tom7misc/trunk/wishlist/}} I have attempted to
present it in a tutorial style that assumes little mathematical or
computer science background. I have also left off the last {\it S} for
{\it Savings}.

\section{First idea: Wishlist}

My website ``{\sf snoot.org}'' has a number of games on it, including
a Scrabble clone called Scribble\footnote{{\tt
http://snoot.org/toys/scribble/}} and Boggle clone called
Muddle.\!\footnote{{\tt http://snoot.org/toys/muddle/}} This website
has been running for almost ten years, comprising over 150,000
Scribble games totaling 3.8 million words placed and 628,000 Muddle
games with over 10 million words found. During each game, players
repeatedly attempt to play words that aren't real. The computer
rebukes them, but hope really springs eternal with these people. It's
like they truly deeply wish to break out of the shackles of the
Official Scrabble Players Dictionary. So the first approach to
determining what words ought to exist is to analyze the words that
people tried to play, in order to try to extract the essence of
word-yearning.

This analysis is quite straightforward. I took the ten years of logs
files and extracted each attempt to play a word in Scribble or Muddle.
These log files are quite large, so the first step is just to get a
count, for each alleged word, and store those in a more convenient
format. There were XXX total words attemped in Scrabble and XXX in
Muddle. The most frequent ones appear in
Figure~\ref{fig:mostfrequent}. The most frequent words are all
legitimate words, since players have a bias towards attempting words
that will not be rebuked by the computer. Seeing the words that people
wish existed is a simple matter of filtering out the words that
already exist, using the Scrabble dictionary. XXX\% of the words
attempted in Scribble and XXX\% in Muddle were not real. The most
frequent ones appear in Figure~\ref{fig:mostfrequentfake}.

\begin{figure}
\caption{Someone needs to fill this in with the data.}
\label{fig:mostfrequent}
\end{figure}

\begin{figure}
\caption{Someone needs to fill this in with the data.}
\label{fig:mostfrequentfake}
\end{figure}

XXX I need to write an analysis of the list and put it here.

In my opinion these are pretty good candidates for words to exist.
Probably if you were playing someone really intense in Scrabble, and
he or she played one of these, and was super deadpan about it and
maybe had caused some crying contests before, and a known
sesquipedalianist, you would let these fly because they look like real
words to me. A point in their favor is that they are quite low-scoring
words in Scrabble; one might expect that the words most wished for are
like ``qzkwv'' for the scoring potential. The effect is probably due
to a few factors: Players are less likely to attempt obvious
non-words, common letters appear more often on the rack and on the
board and so the opportunity to play words like in
Figure~\ref{fig:mostfrequentfake} presents itself more frequently, and
in Muddle, there is no advantage to using unusual letters, except the
joy of being a weirdo. Nonetheless, this list is surely biased by the
specifics of Scribble and Muddle, and the question at hand is not just
what words ought to exist for the purpose of internet word games, but
for general purposes.

\begin{figure}
\caption{Someone needs a diagram or table here of the count histogram.}
\label{fig:gamedistribution}
\end{figure}

Another downside is that this method completely ignores the many words
that are attempted only once or a small number of times. The
distribution is heavy-tailed; XXX\% of the words attempted are not
even in the top XXX (Figure~\ref{fig:gamedistribution}). Though those
words individually are not good candidates to exist, like tiny stars
wished upon in the night sky,\!\footnote{Astronomers now agree that
stars do exist, by the way.} in aggregate they form a significant
planetarium that may tell us what {\it kind} of words people wish
existed. For example, if we saw that the words ``sweeeeeeet'',
``sweeeeeeeeeeeeet'', ``sweeeet'' and ``sweeeeeeeeeeeeeet'' occurred a
few times each, we could infer that people wished that words like
``sweet'' with strictly more than two {\it e}s were real words. They
might even be indifferent to the absolute number of {\it e}s, as long
as there existed some legal variation with more than two {\it e}s.
(This appears to be borne out by data. According to Google's
estimates, the words ``swe$^n$t'' for various medium-sized $n$
(10--20) appear on the Internet with similar frequency. The only
exception is ``sweeeeeeeeeeeeeeeeeeet'', with 19 {\it e}s, which
unexpectedly appears three times as often as 18 or 20 {\it e}s does.)
In order to lance these two boils, in the next section I explore
statistical methods for generalizing from lots of individual examples.

% XXX should really try to get this in the right column, since it
% overflows the margins
\begin{figure}
\begin{tabular}{r|r|l}
17,900,000 & 0 & swt \\
1,060,000 & 1 & swet \\
580,000,000 & 2 & sweet \\
1,310,000 & 3 & sweeet \\
806,000 & 4 & sweeeet \\
509,000 & 5 & sweeeeet \\
283,000$^1$ & 6 & sweeeeeet \\  % spell correction for five
170,000 & 7 & sweeeeeeet \\
115,000 & 8 & sweeeeeeeet \\
75,200 & 9 &  sweeeeeeeeet \\
94,300$^2$ & 10 & sweeeeeeeeeet \\ % spell correction for nine?
51,700 & 11 & sweeeeeeeeeeet \\
37,900 & 12 & sweeeeeeeeeeeet \\
32,000 & 13 & sweeeeeeeeeeeeet \\
25,300 & 14 & sweeeeeeeeeeeeeet \\
24,300 & 15 & sweeeeeeeeeeeeeeet \\
41,000$^3$ & 16 & sweeeeeeeeeeeeeeeet \\ % spell correction for 14
55,000 & 17 & sweeeeeeeeeeeeeeeeet \\
45,000 & 18 & sweeeeeeeeeeeeeeeeeet \\
133,000$^4$ & 19 & sweeeeeeeeeeeeeeeeeeet \\ % for 15
34,800 & 20 &  sweeeeeeeeeeeeeeeeeeeet \\
%
16,100$^5$ & 25 & sweeeeeeeeeeeeeeeeeeeeeeeeet \\ % for weeeeeeeeeeeeeeeeeeeeeeeee t
10,100 & 30 & sweeeeeeeeeeeeeeeeeeeeeeeeeeeeeet \\
2,800 & 40 &  sweeeeeeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
923 & 50 &    sweeeeeeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
118 & 75 &    sweeeeeeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
38 & 100 &    sweeeeeeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\
?$^6$ & 200 &     sweeeeeeeeeeeeeeeeeeeeeeeeeeeeee\ldots t \\ % word is too long for Google
\end{tabular}
\caption{Frequency of ``swe$^n$t'' on the internet for various $n$, estimated
by Google. Notes: {\bf (1)} Spell correction offered for ``sweeeeet''. {\bf (2, 3, 4)} Spell corrections
offered to e$^{9}$, e$^{14}$ and e$^{15}$ respectively. {\bf (5)} Spell correction offered for ``weeeeeeeeeeeeeeeeeeeeeeeee t'' (?) {\bf (6)} With two hundred {\it e}s, the word is too long for Google, which asks me to ``try using a shorter word.'' Thanks Google, but I already did try the shorter ones.}
\label{ref:sweet}
\end{figure}

\section{Statistical Models}

The reason that people are more likely to play words like ``tane'' is
that the letters are common---they appear more often in words, and
more often in the Scrabble bag. But it's not simply a matter of the
frequency of letters; if it were, we would expect to see words like
``eee'' dominating the list, since {\it e} is the most common letter
in English.\!\footnote{Tied for first place with {\it n}, {\it g},
{\it l}, {\it i}, {\it s}, and {\it h}.} People do not play such words
often because they do not {\it seem} like real words. ``oilsoap''
seems more like a word than ``ioaopsl'' to most non-crazy people,
even though they contain the same letters. This is because we have
expectations on what letters are likely to appear next to one
another in words. This section is about modeling expectations on
what letters appear together, and then using that model to generate
the most likely words that don't yet exist.

{\bf Markov chains.}\,
This guy called Andrei Markov had an idea which is pretty obvious in
retrospect, but he had it like a hundred years ago before any of us
were born (probably; if not: you are old), which he didn't call Markov
chains but now they're called Markov chains because I guess in the
hopes that contemporary mathematicians will get stuff named after
their dead selves if they keep the tradition of naming stuff after
dead people alive. The idea is easiest to understand in the context
of the current problem. Suppose we know that the words ``hello'',
``helpful'' and ``felafel'' are the only real words. The following
is a frequency table of how often each letter occurs.

\vspace{0.2in}
\begin{centering}
\begin{tabular}{|r|r|r|r|r|r|r|r|} % {helopfua}
\hline
h   & e   & l   & o   & p   & f   & u   & a    \\
\hline
2   & 4   & 6   & 1   & 1   & 3   & 1   & 1    \\
\hline
\end{tabular}
\end{centering}
\vspace{0.2in}

This tells us that {\it l} is by far the most common letter, so the
most likely word is probably ``l'' or ``llllllll'' or something. A
Markov chain is like a frequency table, but instead of counting
individual letters, we count how often one letter comes {\it after}
another. Here is the Markov chain for those words.

% of the current problem. Suppose we know that the words ``hello'',
% ``helpful'' and ``felafel'' are the only real words. The following

\vspace{0.2in}
\begin{centering}
\begin{tabular}{|c|r|r|r|r|r|r|r|r|} % {helopfua}
\hline
\,  &  {\bf h}   & {\bf e}   & {\bf l}   & {\bf o}   & {\bf p}   & {\bf f}   & {\bf u}   & {\bf a} \\
\hline  %    h     e     l     o     p     f     u     a
{\bf h}   &  0   & 0   & 0   & 0   & 0   & 0   & 0   & 0    \\
\hline
{\bf e}   &  2   & 0   & 0   & 0   & 0   & 2   & 0   & 0    \\
\hline
{\bf l}   &  0   & 4   & 1   & 0   & 0   & 0   & 1   & 0    \\
\hline
{\bf o}   &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline
{\bf p}   &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline
{\bf f}   &  0   & 0   & 0   & 0   & 1   & 0   & 0   & 1    \\
\hline
{\bf u}   &  0   & 0   & 0   & 0   & 0   & 1   & 0   & 0    \\
\hline
{\bf a}   &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline
\end{tabular}
\end{centering}
\vspace{0.2in}

The letters across the top are the ``previous letter'' and the ones
across the left are the ``next letter'' and the box contains the
corresponding count. For example, the pair ``el'' appears four times.
(Pairs of letters are called ``bigrams'' by nerds, some nerd-poseurs,
and Markov who I can't tell if he was a nerd by his picture, because
he does have a pretty austere beard, but also did a lot of math.) One
of the useful things about a Markov chain is that it lets us predict
the next letter that we might see. For example, if we see ``half'',
then the column labeled {\bf f} above tells us that the next letter is
twice as often an {\it e} than a {\it u}, and that no other letters
ever occurred. Typically we think of these as being probabilities
inferred from our observations, so we say there's a 2/3 chance of {\it
e} following {\it f} and a 1/3 chance of {\it u}. Now the word ``llllll''
isn't so likely any more, because there's only a 1/4 chance of the
next letter being {\it l} once we see {\it l}.

Words are not just their interiors; it's also important what letters
tend to start and end words. We can do this by imagining that each
word starts and ends with some fake letters, and include those in the
Markov chain. Let's use {\bf \<} for the start symbol and {\bf \>} for
the end. So we pretend we observed ``\<hello\>'', ``\<helpful\>'', and
``\<felafel\>''. Speaking of which, could you imagine if there were such
a thing as a helpful felafel? Would you eat it? Because then it
probably can't help you any more, except to get fat.

\vspace{0.2in}
\begin{centering}
\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|} % {helopfua}
\hline
\,  &  {\bf \<}    & {\bf h}   & {\bf e}   & {\bf l}   & {\bf o}   & {\bf p}   & {\bf f}   & {\bf u}   & {\bf a}    \\
\hline  %  \<     h     e     l     o     p     f     u     a
{\bf h} &  2  &  0   & 0   & 0   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf e} &  0  &  2   & 0   & 0   & 0   & 0   & 2   & 0   & 0    \\
\hline    
{\bf l} &  0  &  0   & 4   & 1   & 0   & 0   & 0   & 1   & 0    \\
\hline    
{\bf o} &  0  &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf p} &  0  &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf f} &  1  &  0   & 0   & 0   & 0   & 1   & 0   & 0   & 1    \\
\hline    
{\bf u} &  0  &  0   & 0   & 0   & 0   & 0   & 1   & 0   & 0    \\
\hline    
{\bf a} &  0  &  0   & 0   & 1   & 0   & 0   & 0   & 0   & 0    \\
\hline    
{\bf \>} &  0  &  0   & 0   & 2   & 1   & 0   & 0   & 0   & 0    \\
\hline
\end{tabular}
\end{centering}
\vspace{0.2in}

We just added these like other letters, but since the beginning symbol
{\bf \<} never occurs after other letters, we don't need a row for it
(it would be all zeroes), and similarly since no letters ever follow
{\bf \>} we don't need a column for it. Now the word ``lllll'' is
impossible because no words start with {\it l}.

\begin{verbatim}
0.166666667 hel
0.166666667 helpful
0.166666667 hello
0.083333333 felpful
0.083333333 fello
0.083333333 fel
0.041666667 helafel
0.041666667 helafelpful
0.041666667 helafello
\end{verbatim}

% laplace smoothing?

most people remember induction, perhaps 

coinduction: never gonna give you up. once you pop you can't stop.

\section{Ockham's Regexp?}

\section{Survey}

On occasion I have been accused of ``overthinking'' 

Rob: etsy, nuuog
Chris: tapping on the bisphenol-A, nurm
David: wafflucinations
Lea: hnfff sound

In order to not reprint everyone's bullshit---but not introduce bias by selectively removing data---I discarded random subsets of the data until it did not contain bullshit any more.

\section{Recommendations}

sweeeeeeeeeeeeeeeeeeet with 19 {\it e}s.

% \begin{Verbatim}
facebook wall posts, 4-grams
100 top paths:
0.002525504 pittsburgh
0.002093510 steelers
0.002093510 sfo
0.001953943 it's
0.001256106 bdl
0.001116539 sigbovik
0.001099196 facebook
0.000976971 mic
0.000976971 s
0.000976971 can't
0.000837404 i'm
0.000837404 icfp
0.000837404 app
0.000697837 x
0.000697837 drunj
0.000697837 g
0.000616021 ther
0.000558269 doesn't
0.000558269 's
0.000558269 sr
0.000558269 pm
0.000558269 fyi
0.000558269 wtf
0.000558269 k
0.000558269 don't
0.000558269 today's
0.000558269 m
0.000558269 what's
0.000456766 iphone
0.000429438 inted
0.000418702 oh
0.000418702 phl
0.000418702 gonna
0.000418702 pre
0.000418702 zrh
0.000418702 th
0.000418702 h
0.000418702 b
0.000418702 n
0.000418702 ok
0.000418702 d
0.000418702 that's
0.000418702 rex
0.000418702 fanzibar
0.000418702 st
0.000418702 kinda
0.000418702 didn't
0.000355262 cally
0.000334962 brillobox
0.000314027 superburrito
0.000305304 weath
0.000279135 techno
0.000279135 vii


trigrams, including comments:
100 top paths:
0.003013897 it's
0.001856422 ther
0.001704432 i'm
0.001503910 sfo
0.001102867 -
0.001102867 oh
0.001050350 don't
0.001002607 haha
0.001002607 s
0.000902346 bdl
0.000808769 mic
0.000808769 that's
0.000776981 pittsburgh
0.000705812 thes
0.000701825 ok
0.000645063 can't
0.000601564 icfp
0.000601564 k
0.000583335 didn't
0.000507201 runj
0.000501303 g
0.000501303 omg
0.000501303 x
0.000485262 ins
0.000453126 sout
0.000450580 ands
0.000449950 coff
0.000425932 reat
0.000425133 ning
0.000416189 pittle
0.000412468 getty
0.000407151 ster
0.000401043 sr
0.000401043 pm
0.000401043 m
0.000401043 wtf
0.000401043 aw
0.000401043 d
0.000401043 fyi
0.000401043 ple
0.000401043 's
0.000400791 phon
0.000386315 weat
0.000384116 befor
0.000375978 els
0.000374307 als
0.000357782 nothe
0.000354769 app
0.000354769 appy
0.000354422 they're
0.000353750 wate
0.000345727 there's
0.000340508 res
0.000339344 mes
0.000334202 ove
0.000326473 wher
0.000324373 lity
0.000319011 hotos
0.000316613 dea
0.000315777 sigbovik
0.000315105 ver
0.000314965 thand
0.000310607 outh
0.000300782 n


facebook, n = 2
100 top paths:
0.010628671 ing
0.007144197 th
0.005025442 st
0.004291375 re
0.002932937 ne
0.002662094 se
0.002546194 whe
0.002108786 ther
0.001923496 al
0.001669458 con
0.001652820 wor
0.001498222 ch
0.001495485 wis
0.001379778 i'm
0.001336166 jus
0.001302469 sh
0.001280888 yout
0.001244352 fo
0.001225250 le
0.001224422 pre
0.001221357 ar
0.001201881 com
0.001200574 ge
0.001165417 thes
0.001116295 som
0.001108305 fing
0.001104851 ang
0.001102867 -
0.001102706 it's
0.001039704 ist
0.001036886 hing
0.001007930 ou
0.001002607 s
0.000967282 de
0.000932965 en
0.000926568 tor
0.000827260 wat
0.000827151 oh
0.000814235 cor
0.000812961 ned
0.000783696 lis
0.000762020 ple
0.000760162 whis
0.000759898 sed
0.000755396 res
0.000728412 mor
0.000728049 ong
0.000727818 don
0.000720098 cand
0.000710277 ning
0.000708380 ithe
0.000693547 ame
0.000659248 ty
0.000656211 dow
0.000635771 ha
0.000633225 ext
0.000601564 k
% \end{Verbatim}

% Paper ends with a bigram summary of itself.

% Paper ends with a word that is obviously missing a final 's'.


\end{document}
